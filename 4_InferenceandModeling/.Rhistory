# Make a time series plot of ko_pep
plot.zoo(ko_pep)
install.packages("zoo")
library(zoo)
ko <- read.csv(file="./Data/ko.csv", header=TRUE, sep=";")
pep <- read.csv(file="./Data/pep.csv", header=TRUE, sep=";")
# Define ko_pep
ko_pep <- ko/pep
# Make a time series plot of ko_pep
plot.zoo(ko_pep)
View(ko)
ko <- read.csv(file="./Data/ko.csv", header=TRUE, sep=",")
pep <- read.csv(file="./Data/pep.csv", header=TRUE, sep=",")
# Define ko_pep
ko_pep <- ko/pep
# Make a time series plot of ko_pep
plot.zoo(ko_pep)
View(ko)
View(ko)
str(ko)
class(ko)
ko <- as.xts(ko)
ko <- as.zoo(ko)
class(ko)
ko <- read.csv(file="./Data/ko.csv", header=TRUE, sep=",")
pep <- read.csv(file="./Data/pep.csv", header=TRUE, sep=",")
ko <-  as.zoo(ko)
pep <- as.zoo(pep)
# Define ko_pep
ko_pep <- ko/pep
ko <- read.csv(file="./Data/ko.csv", header=TRUE, sep=",")
pep <- read.csv(file="./Data/pep.csv", header=TRUE, sep=",")
ko <-  as.zoo(ko)
pep <- as.zoo(pep)
ko
ko <- read.csv(file="./Data/ko.csv", header=TRUE, sep=",")
pep <- read.csv(file="./Data/pep.csv", header=TRUE, sep=",")
#ko <-  as.zoo(ko)
pep <- as.zoo(pep)
ko
ko <- read.csv(file="./Data/ko.csv", header=TRUE, sep=",", row.names = 1)
pep <- read.csv(file="./Data/pep.csv", header=TRUE, sep=",")
#ko <-  as.zoo(ko)
pep <- as.zoo(pep)
ko
ko <- read.csv(file="./Data/ko.csv", header=TRUE, sep=",", row.names = 1)
pep <- read.csv(file="./Data/pep.csv", header=TRUE, sep=",",row.names = 1)
ko <-  as.zoo(ko)
pep <- as.zoo(pep)
# Define ko_pep
ko_pep <- ko/pep
# Make a time series plot of ko_pep
plot.zoo(ko_pep)
# Add, as a reference a horizontal line at 1
abline(h = 1)
# calculating weights EX
values <- c(500000, 200000, 100000, 20000)
names(values) <- c("Inv 1", "Inv 2", "Inv 3", "Inv 4")
weights <- values/sum(values)
barplot(weights)
# Define the vector values
values <- c(4000, 4000, 2000)
# Define the vector weights
weights <- values / sum(values)
# Print the resulting weights
print(values)
# Define the vector values
values <- c(4000, 4000, 2000)
# Define the vector weights
weights <- values / sum(values)
# Print the resulting weights
print(weights)
# Define marketcaps
marketcaps <- c(5, 8, 9, 20, 25, 100, 100, 500, 700, 2000)
# Compute the weights
weighs <- marketcaps / sum(marketcaps)
# Inspect summary statistics
print(weights)
# Create a barplot of weights
barplot(weights)
# Define marketcaps
marketcaps <- c(5, 8, 9, 20, 25, 100, 100, 500, 700, 2000)
# Compute the weights
weights <- marketcaps / sum(marketcaps)
# Inspect summary statistics
print(weights)
# Create a barplot of weights
barplot(weights)
# Define marketcaps
marketcaps <- c(5, 8, 9, 20, 25, 100, 100, 500, 700, 2000)
# Compute the weights
weights <- marketcaps / sum(marketcaps)
# Inspect summary statistics
summary(weights)
# Create a barplot of weights
barplot(weights)
# Vector of initial value of the assets
in_values <- c(1000, 5000, 2000)
# Vector of final values of the assets
fin_values <- c(1100, 4500, 3000)
# Weights as the proportion of total value invested in each assets
weights <- in_values / sum(in_values)
# Vector of simple returns of the assets
returns <- (fin_values - in_values)/in_values
# Compute portfolio return using the portfolio return formula
preturns <- sum(returns*weights)
r1 <- 0.1
r2 <- -0.05
rtn <- (1+r1)*(1+r2)-1
print(rtn)
returns <- Return.calculate(po)
install.packages("PerformanceAnalytics")
library(PerformanceAnalytics)
returns <- Return.calculate(po)
install.packages("zoo")
library(zoo)
ko <- read.csv(file="./Data/ko.csv", header=TRUE, sep=",", row.names = 1)
pep <- read.csv(file="./Data/pep.csv", header=TRUE, sep=",",row.names = 1)
ko <-  as.zoo(ko)
pep <- as.zoo(pep)
# Define ko_pep
ko_pep <- ko/pep
# Make a time series plot of ko_pep
plot.zoo(ko_pep)
# Add, as a reference a horizontal line at 1
abline(h = 1)
# calculating weights EX
values <- c(500000, 200000, 100000, 20000)
names(values) <- c("Inv 1", "Inv 2", "Inv 3", "Inv 4")
weights <- values/sum(values)
barplot(weights)
# Define the vector values
values <- c(4000, 4000, 2000)
# Define the vector weights
weights <- values / sum(values)
# Print the resulting weights
print(weights)
# Define marketcaps
marketcaps <- c(5, 8, 9, 20, 25, 100, 100, 500, 700, 2000)
# Compute the weights
weights <- marketcaps / sum(marketcaps)
# Inspect summary statistics
summary(weights)
# Create a barplot of weights
barplot(weights)
# Vector of initial value of the assets
in_values <- c(1000, 5000, 2000)
# Vector of final values of the assets
fin_values <- c(1100, 4500, 3000)
# Weights as the proportion of total value invested in each assets
weights <- in_values / sum(in_values)
# Vector of simple returns of the assets
returns <- (fin_values - in_values)/in_values
# Compute portfolio return using the portfolio return formula
preturns <- sum(returns*weights)
r1 <- 0.1
r2 <- -0.05
rtn <- (1+r1)*(1+r2)-1
print(rtn)
install.packages("PerformanceAnalytics")
library(PerformanceAnalytics)
returns <- Return.calculate(po)
install.packages("zoo")
returns <- Return.calculate(ko)
install.packages("PerformanceAnalytics")
library(PerformanceAnalytics)
returns <- Return.calculate(ko)
prices <- read.csv(file="./Data/prices.csv", header=TRUE, sep=",",row.names = 1)
#prices <-  as.zoo(prices)
returns <- Return.calculate(prices)
# Delete the first row with NA
returns <- returns[(-1),]
# Load package PerformanceAnalytics
library(PerformanceAnalytics)
# Print the first and last six rows of prices
head(prices)
tail(prices)
# Create the variable returns using Return.calculate()
returns <- Return.calculate(prices)
# Print the first six rows of returns. Note that the first observation is NA, because there is no prior price.
head(returns)
# Remove the first row
returns <- returns[-1, ]
# Create the weights
eq_weights <- c(0.5, 0.5)
# Create a portfolio using buy and hold
pf_bh <- Return.portfolio(returns, weights = eq_weights)
# Load package PerformanceAnalytics
library(PerformanceAnalytics)
# Print the first and last six rows of prices
head(prices)
tail(prices)
# Create the variable returns using Return.calculate()
returns <- Return.calculate(prices)
# Print the first six rows of returns. Note that the first observation is NA, because there is no prior price.
head(returns)
# Remove the first row
returns <- returns[-1, ]
print(returns)
prices <- read.csv(file="./Data/prices.csv", header=TRUE, sep=",",row.names = 1)
prices <-  as.zoo(prices)
returns <- Return.calculate(prices)
# Create the weights
eq_weights <- c(0.5, 0.5)
# Create a portfolio using buy and hold
pf_bh <- Return.portfolio(returns, weights = eq_weights, verbose = TRUE)
# above ex coded up in r
beads <- ep( c("red", "blue"), times = c(2,3))
# above ex coded up in r
beads <- rep( c("red", "blue"), times = c(2,3))
beads
# take a random sample
sample(beads, 1)
install.packages("tidyverse")
install.packages("dslabs")
library(tidyverse)
library(dslabs)
ds_theme_set()
take_poll(25)
# `N` represents the number of people polled
N <- 25
# Create a variable `p` that contains 100 proportions ranging from 0 to 1 using the `seq` function
p <- seq(0, 1, length = 100)
# Create a variable `se` that contains the standard error of each sample average
se <- sqrt(p*(1-p)/N)
# Plot `p` on the x-axis and `se` on the y-axis
plot(p, se)
# The vector `p` contains 100 proportions of Democrats ranging from 0 to 1 using the `seq` function
p <- seq(0, 1, length = 100)
# The vector `sample_sizes` contains the three sample sizes
sample_sizes <- c(25, 100, 1000)
# Write a for-loop that calculates the standard error `se` for every value of `p` for each of the three samples sizes `N` in the vector `sample_sizes`. Plot the three graphs, using the `ylim` argument to standardize the y-axis across all three plots.
for(N in sample_sizes){
se <- sqrt(p*(1-p)/N)
plot(p, se, ylim = c(0,0.5/sqrt(25)))
}
# `N` represents the number of people polled
N <- 25
# `p` represents the proportion of Democratic voters
p <- 0.45
# Calculate the standard error of the spread. Print this value to the console.
2*sqrt(p*(1-p)/N)
# Example: Let's compute this estimate of the standard error for the first sample that we took, in which we had 12 blue beads and 13 red beads.
# In that case, X-bar was 0.48.
N <- 25
X_hat <- 0.48
se <- sqrt(X_hat*(1-X_hat)/N)
se
pnorm(0.01/se) - pnorm(-0.01/se)
pnorm(1) - pnorm(-1)
pnorm(2) - pnorm(-2)
pnorm(3) - pnorm(-3)
B <- 10000
N <- 1000
X_hat <- replicate(B, {
X <- sample(c(0,1), size=N, replace=TRUE, prob=c(1-p, p))
mean(X)
})
# run this sim
p <- 0.45
N <- 1000
X <- sample(c(0,1), size=N, replace=TRUE, prob=c(1-p, p))
X_hat <-  mean(X)
# then use result in a monte carlo sim
B <- 10000
X_hat <- replicate(B, {
X <- sample(c(0,1), size=N, replace=TRUE, prob=c(1-p, p))
mean(X)
})
# Expected 0.45
mean(X_hat)
# Expected 0.015
sd(X_hat)
install.packages("gridExtra")
install.packages("tidyr")
install.packages("ggplot2")
library(ggplot2)
library(tidyr)
library(gridExtra)
p1 <- data.frame(X_hat=X_hat) %>% ggplot(aes(X_hat)) +
geom_histogram(binwidth = 0.005, color="black")
p2 <- data.frame(X_hat=X_hat) %>% ggplot(aes(sample=X_hat)) +
stat_qq(dparams = list(mean=mean(X_hat), sd=sd(X_hat))) +
geom_abline() +
ylab("X_hat") +
xlab("Theoretical normal")
grid.arrange(p1,p2, nrow=1)
N <- 100000
p <- seq(0.35, 0.65, length = 100)
SE <- sapply(p, function(x) 2*sqrt(x*(1-x)/N))
data.frame(p=p, SE=SE) %>% ggplot(aes(p, SE)) +
geom_line()
# Write a function called `take_sample` that takes `p` and `N` as arguements and returns the average value of a randomly sampled population.
take_sample <- function(p, N){
X <- sample(c(0,1), size = N, replace = TRUE, prob = c(1 - p, p))
mean(X)
}
# Use the `set.seed` function to make sure your answer matches the expected result after random sampling
set.seed(1)
# Define `p` as the proportion of Democrats in the population being polled
p <- 0.45
# Define `N` as the number of people polled
N <- 100
# Call the `take_sample` function to determine the sample average of `N` randomly selected people from a population containing a proportion of Democrats equal to `p`. Print this value to the console.
take_sample(p,N)
# Define `p` as the proportion of Democrats in the population being polled
p <- 0.45
# Define `N` as the number of people polled
N <- 100
# The variable `B` specifies the number of times we want the sample to be replicated
B <- 10000
# Use the `set.seed` function to make sure your answer matches the expected result after random sampling
set.seed(1)
# Create an objected called `errors` that replicates subtracting the result of the `take_sample` function from `p` for `B` replications
errors <- replicate(B, p - take_sample(p, N))
# Calculate the mean of the errors. Print this value to the console.
mean(errors)
install.packages("tidyr")
one_poll_per_pollster <- polls %>% group_by(pollster) %>%
filter(enddate == max(enddate)) %>%
ungroup
library(dslabs)
data(polls_us_election_2016)
names(polls_us_election_2016)
polls <- polls_us_election_2016 %>%
filter(state== "U.S." & enddate >= "2016-10-31" &
(grade %in% c("A+", "A", "A-", "B+") | is.na(grade)))
library(tidyverse)
install.packages("tidyverse")
library(tidyverse)
library(dslabs)
data(polls_us_election_2016)
names(polls_us_election_2016)
polls <- polls_us_election_2016 %>%
filter(state== "U.S." & enddate >= "2016-10-31" &
(grade %in% c("A+", "A", "A-", "B+") | is.na(grade)))
# adding a spread estimate
polls <- polls %>%
mutate(spread = rawpoll_clinton/100 - rawpoll_trump/100)
# The estimated spread is now computed like this because now the sample size
d_hat <- polls %>%
summarize(d_hat = sum(spread * samplesize) / sum(samplesize)) %>% .$d_hat
p_hat <- (d_hat+1)/2
moe <- 1.96 * 2 * sqrt(p_hat*(1-p_hat) / sum(polls$samplesize))
moe # margin of error
polls %>%
ggplot(aes(spread)) +
geom_histogram(color="black", binwidth = 0.01)
polls %>% group_by(pollster) %>%
filter(n() >= 6) %>%
ggplot(aes(pollster, spread)) +
geom_point()
theme(axis.text.x = element_text(angle = 90, hjust = 1))
polls %>% group_by(pollster) %>%
filter(n() >= 6) %>%
summarize(se = 2 * sqrt( p_hat * (1-p_hat) / median(samplesize)))
one_poll_per_pollster <- polls %>% group_by(pollster) %>%
filter(enddate == max(enddate)) %>%
ungroup
one_poll_per_pollster <- polls %>% group_by(pollster) %>%
filter(enddate == max(enddate)) %>%
ungroup
# make a histogram
one_poll_per_pollster %>%
ggplot(aes(spread)) + geom_histogram(binwidth=0.01)
sd(one_poll_per_pollster$spead)
sd(one_poll_per_pollster$spread)
results <- one_poll_per_pollster %>%
summarize(avg = mean(spread), se = sd(spread) / sqrt(length(spread))) %>%
mutate(start = avg - 1.96*se, end = avg + 1.96*se)
round(esults*100,1)
results <- one_poll_per_pollster %>%
summarize(avg = mean(spread), se = sd(spread) / sqrt(length(spread))) %>%
mutate(start = avg - 1.96*se, end = avg + 1.96*se)
round(results*100,1)
# Load the 'dslabs' package and data contained in 'heights'
library(dslabs)
data(heights)
# Make a vector of heights from all males in the population
x <- heights %>% filter(sex == "Male") %>%
.$height
# Calculate the population average. Print this value to the console.
mean(x)
# Calculate the population standard deviation. Print this value to the console.
sd(x)
# The vector of all male heights in our population `x` has already been loaded for you. You can examine the first six elements using `head`.
head(x)
# Use the `set.seed` function to make sure your answer matches the expected result after random sampling
set.seed(1)
# Define `N` as the number of people measured
N <- 50
# Define `X` as a random sample from our population `x`
X %>% sample(c(0,1), size=N, eplace = TRUE, prob=c(1-p,p))
# The vector of all male heights in our population `x` has already been loaded for you. You can examine the first six elements using `head`.
head(x)
# Use the `set.seed` function to make sure your answer matches the expected result after random sampling
set.seed(1)
# Define `N` as the number of people measured
N <- 50
# Define `X` as a random sample from our population `x`
X %>% sample(c(0,1), size=N, replace = TRUE, prob=c(1-p,p))
# The vector of all male heights in our population `x` has already been loaded for you. You can examine the first six elements using `head`.
head(x)
# Use the `set.seed` function to make sure your answer matches the expected result after random sampling
set.seed(1)
# Define `N` as the number of people measured
N <- 50
# Define `X` as a random sample from our population `x`
X <- sample(x, N, replace = TRUE)
# Calculate the sample average. Print this value to the console.
mean(X)
# Calculate the sample standard deviation. Print this value to the console.
sd(X)
# The vector of all male heights in our population `x` has already been loaded for you. You can examine the first six elements using `head`.
head(x)
# Use the `set.seed` function to make sure your answer matches the expected result after random sampling
set.seed(1)
# Define `N` as the number of people measured
N <- 50
# Define `X` as a random sample from our population `x`
X <- sample(x, N, replace = TRUE)
# Define `se` as the standard error of the estimate. Print this value to the console.
se <- sd(X)/sqrt(N)
se
# Construct a 95% confidence interval for the population average based on our sample. Save the lower and then the upper confidence interval to a variable called `ci`.
ci <- c(mean(X) - qnorm(0.975)*se, mean(X) + qnorm(0.975)*se)
ci
# The vector of all male heights in our population `x` has already been loaded for you. You can examine the first six elements using `head`.
head(x)
# Use the `set.seed` function to make sure your answer matches the expected result after random sampling
set.seed(1)
# Define `N` as the number of people measured
N <- 50
# Define `X` as a random sample from our population `x`
X <- sample(X, N, replace = TRUE)
# Define `se` as the standard error of the estimate. Print this value to the console.
se <- sd(X)/sqrt(N)
se
# Construct a 95% confidence interval for the population average based on our sample. Save the lower and then the upper confidence interval to a variable called `ci`.
ci <- c(mean(X) - qnorm(0.975)*se, mean(X) + qnorm(0.975)*se)
ci
# Define `mu` as the population average
mu <- mean(x)
# Use the `set.seed` function to make sure your answer matches the expected result after random sampling
set.seed(1)
# Define `N` as the number of people measured
N <- 50
# Define `B` as the number of times to run the model
B <- 10000
# Define an object `res` that contains a logical vector for simulated intervals that contain mu
res <- replicate(B, {
X <- sample(x, N, replace=TRUE)
interval <- mean(X) + c(-1,1)*qnorm(0.975)*sd(X)/sqrt(N)
between(mu, interval[1], interval[2])
})
# Calculate the proportion of results in `res` that include mu. Print this value to the console.
mean(res)
# Load the libraries and data you need for the following exercises
library(dslabs)
library(dplyr)
library(ggplot2)
data("polls_us_election_2016")
# These lines of code filter for the polls we want and calculate the spreads
polls <- polls_us_election_2016 %>%
filter(pollster %in% c("Rasmussen Reports/Pulse Opinion Research","The Times-Picayune/Lucid") &
enddate >= "2016-10-15" &
state == "U.S.") %>%
mutate(spread = rawpoll_clinton/100 - rawpoll_trump/100)
# Make a boxplot with points of the spread for each pollster
polls %>% ggplot(aes(pollster, spread)) +
geom_boxplot() +
geom_point()
# The `polls` data have already been loaded for you. Use the `head` function to examine them.
head(polls)
# Create an object called `sigma` that contains a column for `pollster` and a column for `s`, the standard deviation of the spread
sigma <- polls %>% group_by(pollster) %>%
summarize(s = sd(spread))
# Print the contents of sigma to the console
sigma
# The `polls` data have already been loaded for you. Use the `head` function to examine them.
head(polls)
# Create an object called `res` that summarizes the average, standard deviation, and number of polls for the two pollsters.
res <- polls %>% group_by(pollster) %>%
summarize(avg = mean(spread), s = sd(spread), N = n())
# Store the difference between the larger average and the smaller in a variable called `estimate`. Print this value to the console.
estimate <- res$avg[2] - res$avg[1]
estimate
# Store the standard error of the estimates as a variable called `se_hat`. Print this value to the console.
se_hat <- sqrt(res$s[2]^2/res$N[2] + res$s[1]^2/res$N[1])
se_hat
# Calculate the 95% confidence interval of the spreads. Save the lower and then the upper confidence interval to a variable called `ci`.
ci <- c(estimate - qnorm(0.975)*se_hat, estimate + qnorm(0.975)*se_hat)
# We made an object `res` to summarize the average, standard deviation, and number of polls for the two pollsters.
res <- polls %>% group_by(pollster) %>%
summarize(avg = mean(spread), s = sd(spread), N = n())
# The variables `estimate` and `se_hat` contain the spread estimates and standard error, respectively.
estimate <- res$avg[2] - res$avg[1]
se_hat <- sqrt(res$s[2]^2/res$N[2] + res$s[1]^2/res$N[1])
# Calculate the p-value
2*(1 - pnorm(estimate/se_hat, 0, 1))
# Execute the following lines of code to filter the polling data and calculate the spread
polls <- polls_us_election_2016 %>%
filter(enddate >= "2016-10-15" &
state == "U.S.") %>%
group_by(pollster) %>%
filter(n() >= 5) %>%
mutate(spread = rawpoll_clinton/100 - rawpoll_trump/100) %>%
ungroup()
# Create an object called `var` that contains columns for the pollster, mean spread, and standard deviation. Print the contents of this object to the console.
var <- polls %>% group_by(pollster) %>%
summarize(avg = mean(spread), s = sd(spread))
var
